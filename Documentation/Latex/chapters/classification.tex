\section{Classification}
After the preprocessing phase, the dataset is ready to be used to learn regression models that will be used in the final application. In the following chapter all the chosen strategies are discussed, and the results are compared.



\subsection{Strategies}
\subsubsection{Train and Test Splitting}
All the classifiers have been evaluated with the same strategy: \textit{10-fold cross validation}. This means that a model is learned from the folds of the training set (90\%) and they are evaluated against the corresponding test fold (10\%); then the operation is repeated 10 times, and at each iteration a different fold acts as test set.

Despite the fact that Weka provides the \textbf{Evaluation.crossValidateModel()} method, we wanted to save the results of every fold and to have access to the selected attributes in case of algorithm with feature selection.

For these reasons, instead of using a meta classifier, we manually split training set and test set through the \textit{Weka} provided functions \textbf{Instances.trainCV(numFolds, currentFold)} and \textbf{Instances.testCV(numFolds, currentFold)} according to the \textit{WekaWiki tutorial} on how to manually construct a \textit{k-fold cross validation}; then the \textit{attribute selection} algorithms (when present) have been built exclusively on the training set but applied both to training and to test set.

Finally, we saved the results of each fold, but since we need only a model of the classifier while this procedure generates 10 different models, we picked the one with the lowest \textit{root mean squared error}, aware that this does not necessarily means that we chose the best one



\subsubsection{Chosen Classifiers}
According to our application domain, the classifiers needed are actually regression algorithms capable of handling our non-nominal class. The selected algorithms have been tested both on the full dimensionality of the dataset (138 features) and on the reduced subspaces identified by the supervised attribute selection methods. Once again, we underline the fact that those features selection algorithms have been modeled exclusively on the bases of the training set. They are \textit{CfsSubsetEval+BestFirst} and \textit{CfsSubsetEval+GreedyStepwise}.

We couldn’t exploit the \textit{InfoGain} evaluator since it is not capable of working with numerical classes; we discarded \textit{PCA} since it works transforming dimensions, thus it would be difficult to understand if we could have afforded to ask only for a limited number of parameters to input from the user in the final application (\textbf{e.g.}, if \textit{CfsSubsetEval+BestFirst} selects only 3 attributes, we can create an application that requires only 3 parameters as input. With \textit{PCA} we don’t know which original features has been chosen in order to generate the new space, so we are forced to ask the user to input alle the 138 parameters); eventually, we discard the use of a wrapped classifier in order not to deteriorate much the performance of the application.

To summarize, the tested classifiers are:
\begin{itemize}
	\item \textit{Linear Regression, Linear Regression with attribute selection}
	\item \textit{Random Forest, Random Forest with attribute selection}
	\item \textit{5-NN, 5-NN with attribute selection}
	\item \textit{M5Rules with and without attribute selection}
\end{itemize}




\subsection{Building Classification Models}
On the following pictures the implementation of the classifiers is reported.
The main method is not here shown, it simply loads data and triggers the algorithm. A multithreaded approach has been tested but not implemented since the huge amount of principal memory consumption due to the model building made the system fail more than once.

\subsubsection{Data Loading}
\begin{lstlisting}[language=Java]
public Instances loadData() {
	Instances data=null;
        try {
            ConfigurationData configurationData = getConfigData();
            CSVLoader loader= new CSVLoader();
            loader.setDateAttributes(configurationData.dateAttributes);
            loader.setDateFormat(configurationData.dateFormat);
            loader.setEnclosureCharacters("\"");
            loader.setFieldSeparator(",");
            loader.setMissingValue("?");
            loader.setNominalAttributes(configurationData.nominalAttributes);
            loader.setNumericAttributes(configurationData.numericAttributes);
            loader.setSource(new File(datasetPath));
            data=loader.getDataSet();
        } catch (Exception e){
            e.printStackTrace();
        }
        return data;
}

private ConfigurationData getConfigData() throws JAXBException {
        JAXBContext jaxbContext = JAXBContext.newInstance(ConfigurationData.class);
        Unmarshaller jaxbUnmarshaller = jaxbContext.createUnmarshaller();
        ConfigurationData conf = (ConfigurationData) jaxbUnmarshaller.unmarshal(new File(confDataPath));
        System.out.println(conf.dateAttributes + "\n" + conf.dateFormat + "\n" + conf.numericAttributes);
        return conf;
}
\end{lstlisting}

\textbf{DatasetFromCsvLoader.java} is the class in charge of loading the dataset stored as a \textit{CSV file}. It simply uses the \textit{Weka CSVLoader} but the options have been saved in a configuration file, so that to maximize the separation of concerns between the \textit{Weka} internal representation of the data and our code.

\subsubsection{Classifier Definition}
\begin{lstlisting}[language=Java]
private void buildLinearRegressionWithAttributeSelection() {
        try{
            Instances randData = new Instances(dataset);
            randData.randomize(new Random(1));

            for(int i=0; i<numFolds; i++){
                AttributeSelection filter = new AttributeSelection();
                filter.setEvaluator(new CfsSubsetEval());
                filter.setSearch(new BestFirst());
                executeCV(randData, i, filter, "CfsSubsetEval+BestFirst");
            }

            randData = new Instances(dataset);
            randData.randomize(new Random(1));

            for(int i=0; i<numFolds; i++){
                AttributeSelection filter = new AttributeSelection();
                filter.setEvaluator(new CfsSubsetEval());
                filter.setSearch(new GreedyStepwise());
                executeCV(randData, i, filter, "CfsSubsetEval+GreedyStepwise");
            }

            System.out.println("linear regression with attribute selection terminated");

        } catch (Exception e) {
            e.printStackTrace();
        }
}
\end{lstlisting}

Every algorithm is implemented in a separate class, which contains different methods based on the way we want to test it (\textit{with} or \textit{w/o} attribute selection). In the code shown the classifier definition is reported: after a randomization of the dataset, we call \textit{numFolds} time the \textbf{executeCV()} that performs a single round of the \textit{cross-validation}.

\subsubsection{Classifier Implementation}
\begin{lstlisting}[language=Java]
private void executeCV(Instances dataset, int currentFold, AttributeSelection filter, String filterName) throws Exception {
        Instances train = dataset.trainCV(numFolds, currentFold);
        Instances test = dataset.testCV(numFolds, currentFold);
        List<Attribute> chosen=new ArrayList<>();
        if(filter!=null) {
            filter.setInputFormat(train);
            Instances trainReduced = Filter.useFilter(train, filter);
            Instances testReduced = Filter.useFilter(test, filter);
            train = new Instances(trainReduced);
            test = new Instances(testReduced);
            for(int i=0; i<train.numAttributes(); i++)
                chosen.add(train.attribute(i));
        }
        weka.classifiers.functions.LinearRegression classifier = new weka.classifiers.functions.LinearRegression();
        classifier.buildClassifier(train);
        Evaluation evaluation = new Evaluation(train);
        evaluation.evaluateModel(classifier, test);
        new FileSaver(evaluation, "LinearRegression", filterName, currentFold, chosen).save();
        if(currentFold==8)
            saveModel(filterName, classifier, test);
    }

private void saveModel(String filterName, weka.classifiers.functions.LinearRegression classifier, Instances dataFormat) throws Exception{
        SerializationHelper.write("models/LinearRegression_" + filterName + ".model", classifier);
        ArffSaver saver = new ArffSaver();
        saver.setInstances(dataFormat);
        saver.setFile(new File("models/LinearRegression_" + filterName + "_data.arff"));
        saver.writeBatch();
    }
\end{lstlisting}

In this method we perform a round of the \textbf{10-fold cross validation}, in the way suggested by the \textit{WekaWiki tutorial}. At each iteration, the \textit{trainCV} and \textit{testCV} return non-overlapping training sets and test sets; if an attribute selection method has been defined, it is built on top of the training set and applied to both of them and the list of the chosen attributes is stored. In the end, the model built on the \textbf{fold n.8} is stored in a \textit{.model} file. 




\subsection{Performance Evaluation and Effects of Attribute Selection}
Once the classifiers have been built, as anticipated in the par. 3.1 we tested them using a \textbf{10-fold cross validation}. 
In the following table we report the average values respectively of \textit{correlation coefficient (CC), mean absolute error (mae), root mean squared error (rmse), relative absolute error (rae)} and \textit{root relative squared error (rrse)} for each tested classifier (note that the chosen model, \textit{i.e.} the one built in the $8^{th}$ fold, shows better measures than the average for every tested classifier).
The green-colored values are the best ones in absolute.


\begingroup
\setlength{\tabcolsep}{6pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.5} % Default

	\begin{longtable}{| m{18em} | m{5em} | m{8em} | m{8em} |} 
		\hline
		\textbf{ATTIBUTE SELECTION $\Rightarrow$ \newline REGRESSION ALGORITHM $\Downarrow$} & \textbf{None} & \textbf{CfsSubsetEval + BestFirst} & \textbf{CfsSubsetEval + GreedyStepwise} \\
		\hline
		\textbf{Linear Regression} & Out of memory error &  $\overline{CC}=0,6793 \newline \overline{Mae}=40,0323 \newline \overline{Rmse}=57,8399 \newline \overline{Rae}=66,9399 \newline \overline{Rrse}=73,3966$ & 0,6794 \newline 40,0308 \newline 57,8339 \newline 66,9375 \newline 73,3891 \\
		\hline
		\textbf{Random Forest} & 0.7495 \newline36,0881 \newline52,8727 \newline 60,3423 \newline 67,0864 & 0.6988 \newline
37,9716 \newline
56,3332 \newline
63,4916 \newline
71,7302 &
0,7000 \newline
37,9451\newline
56,5060\newline
63,4470\newline
71,6955\\
	\hline
	\textbf{5-NN} & 0,5413 \newline
46,4003\newline
67,0074\newline
77,2522\newline
84,3622 &
0,6772\newline
39,0709\newline
58,0759\newline
65,3630\newline
73,6845 &
0,6772\newline
39,0709\newline
58,0759\newline
65,3630\newline
73,6845 \\
	\hline
	\textbf{M5Rules} & 
0,6589\newline
567,0975\newline
31.491,7374\newline
942,2971\newline
39.372,3750 &
0,6942\newline
38,5873\newline
56,7530\newline
64,5234\newline
72,0102 &
0,6942\newline
38,5872\newline
56,7498\newline
64,5211\newline
72,0100 \\
	\hline

	\end{longtable}

\endgroup


As shown in the table, \textit{Random Forest} without attribute selection is the algorithm that performs best \textbf{w.r.t.} all the parameters. Note that the attribute selection improved the outcomes of certain algorithms, anyway, deteriorating the ones of \textit{Random Forest}.
Now we will discuss the pros and cons of each of them, justifying the final choice for our application:

\begin{enumerate}
	\item All the classifiers without attribute selection are, unfortunately, not very suitable for the usage of the application: they would require the user to input 288 parameters, which is a huge amount of work to do for him/her. On the contrary, all the attribute selected classifier needs less than 30 features.
	\item \textit{Linear Regression} shows good but not awesome results, it requires not much memory for the model loading and it is very fast at prediction time. However, it showed very long times and high memory consumption at training time.
	\item \textit{Random Forest} is the one with the best results in absolute, both with and without attribute selection. It is quite fast both to train and to use for prediction, anyway it needs a lot of time and memory for the model to be saved and loaded. Since the considerations at the point 0), we discarded the pure \textit{Random Forest} and we chose \textit{Random Forest with CfsSubsetEval+GreedyStepwise} as the default regression model for our application.
	\item \textit{K-NN} is definitely inadequate, although it requires very little time and memory for the model to be built (and then also to be loaded), the computational effort is concentrated at prediction time (thus increasing the application response time), and it also shows quite poor results.
	\item \textit{M5Rules} has very good performances and it needs very little time and memory to save and load the model. The prediction is not so fast, the time required to build the model is terribly long (up to 3 or 4 times more than Random Forest). Anyway, since the majority of the effort is at training time, it is suitable for our application, thus \textit{M5Rules with CfsSubsetEval+GreedyStepwise} has been chosen as backup regression model.
\end{enumerate}

\subsection{Evaluation of Significance of the Classifiers' Results}
Once evaluated the performance of the algorithms, the main question is if we can consider those differences statistically significant or not. In order to make such a computation, we used the paired \textbf{Student’s t-test} on the \textit{RMSE} considering all the outcomes of all the folds (of every algorithm we chose only the \textit{CfsSubsetEval+ GreedyStepwise} version). The results are the following ($sig\_rate=5\%, deg=9$):

\begin{itemize}
	\item \textit{Random Forest} \textbf{w.r.t.} \textit{Linear Regression}: t=5.5342; p=0.004 $\Rightarrow$ stat. sign.
	\item \textit{Random Forest} against \textit{M5Rules}: t=0.805; p=0.4413 $\Rightarrow$ non stat. sign.
	\item \textit{M5Rules} against \textit{Linear Regression}: t=5.5104; p=0.032  $\Rightarrow$  stat. sign.
	\item \textit{Linear Regression} against \textit{KNN}: t=0.6601; p=0.5257 $\Rightarrow$ non stat. sign
\end{itemize}

 \textit{Random Forest} and \textit{M5Rules} are hence comparable, so they are equally valid for our application. On the contrary, the \textbf{t-test} shows that  \textit{Linear Regression} 's results are much more similar to \textit{KNN} ones rather than to the outcomes of the other two algorithms, thus it has been discarded.